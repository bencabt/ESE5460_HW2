# train.py
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from allcnn import allcnn_t
from torchvision.transforms import ToTensor
import torch.optim as optim
from datetime import datetime
import requests
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt




#Text URLs
text1URL = "https://www.gutenberg.org/cache/epub/100/pg100.txt"
text2URL = "https://www.gutenberg.org/cache/epub/2600/pg2600.txt"
text3URL = "https://www.gutenberg.org/cache/epub/77109/pg77109.txt"

# Download texts
response1 = requests.get(text1URL)
response2 = requests.get(text2URL)
response3 = requests.get(text3URL)

# Get text content as strings
text = response1.text + " " + response2.text + " " + response3.text

#Get number of unique characters
all_chars = sorted(list(set(text)))
m = len(all_chars)
print(f"Number of unique characters: {m}")
print(enumerate(all_chars))

#character index mapping
char_to_idx = {ch: i for i, ch in enumerate(all_chars)}
idx_to_char = {i: ch for i, ch in enumerate(all_chars)}

#Create tensor
def text_to_tensor(text, seq_len=33):
    sequences = []
    for i in range(0, len(text) - seq_len, seq_len):
        seq = text[i:i+seq_len]
        one_hot = np.zeros((seq_len, m), dtype=np.float32)
        for t, ch in enumerate(seq):
            one_hot[t, char_to_idx[ch]] = 1.0
        sequences.append(one_hot)
    full_sequence = np.stack(sequences)
    modified_sequence = full_sequence[1:-1]
    return modified_sequence

#shuffle data
def shuffle_data(sequence, seed=123):
    np.random.seed(seed)
    indices = np.arange(len(sequence))
    np.random.shuffle(indices)
    return sequence[indices]

#train test split
def train_val_split(data, val_fraction=0.2):
    val_size = int(len(data) * val_fraction)
    train_data = data[:-val_size]
    val_data = data[-val_size:]
    return train_data, val_data

#split x and y, where y is next character, y is flattened indices
def xy_split(data):
    X = data[:, :-1, :]
    y = data[:, 1:, :]
    y_flat = np.argmax(y, axis=2)
    return X, y_flat

#Prepare data
shuffled_data = shuffle_data(text_to_tensor(text, seq_len=33))
train_data, val_data = train_val_split(shuffled_data, val_fraction=0.2)

#Split data
train_X, train_y = xy_split(train_data)
val_X, val_y = xy_split(val_data)

#Create tensor
train_X_tensor = torch.tensor(train_X, dtype=torch.float32)
train_Y_tensor = torch.tensor(train_y, dtype=torch.long) # Use torch.long for indices
val_X_tensor = torch.tensor(val_X, dtype=torch.float32)
val_Y_tensor = torch.tensor(val_y, dtype=torch.long)     # Use torch.long for indices

#Create Datasets
train_dataset = TensorDataset(train_X_tensor, train_Y_tensor) 
val_dataset = TensorDataset(val_X_tensor, val_Y_tensor)


#Create DataLoaders
batch_size = 128
trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
testloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

#Set up device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

#Define RNN model
class CharRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(CharRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    #this is a 32 sequence task
    def forward(self, x):
        out, _ = self.rnn(x)
        out_flat = out.reshape(-1, out.shape[2])
        logits_flat = self.fc(out_flat)
        logits = logits_flat.reshape(out.shape[0], out.shape[1], logits_flat.shape[1])
        return logits

#training
def train(dataloader, model, loss_fn, optimizer, iter):
    size = len(dataloader.dataset)
    model.train() #set model to training mode

    #training loop
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        #Compute prediction and loss
        pred = model(X)
        #flatten for loss calc
        pred_flat = pred.reshape(-1, pred.shape[2])
        y_flat = y.reshape(-1)
        
        loss = loss_fn(pred_flat, y_flat)

        #Backpropagation
        optimizer.zero_grad() #zero gradients
        loss.backward() #calculate gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        optimizer.step() #update weights

        iter[0] += 1 #iter global step

        if batch % 100 == 0: #print out loss every 100 batches
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

#Validation
def test(dataloader, model, loss_fn):
    total_characters = len(dataloader.dataset) * dataloader.dataset.tensors[0].shape[1] #sequence * 32
    num_batches = len(dataloader)
    model.eval() #set model to evaluation mode
    test_loss, correct = 0, 0

    with torch.no_grad(): #no need to track gradients
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            pred_flat = pred.reshape(-1, pred.shape[2])
            y_flat = y.reshape(-1)
            #loss
            test_loss += loss_fn(pred_flat, y_flat).item()
            #accuracy
            correct += (pred.argmax(1) == y_flat).type(torch.float).sum().item()

    test_loss /= num_batches
    correct /= total_characters
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

    return test_loss, correct


#Set up model
hidden_size = 128
learning_rate = 1e-3
epochs = 10
model = CharRNN(input_size=m, hidden_size=hidden_size, output_size=m).to(device)
print(model)

#Loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

iter = [0] #global step
result_plot = []

#Training loop
print("COMMENCING TRAINING...\n")
for epoch in range(epochs):
    print(f"Epoch {epoch+1}\n-------------------------------")
    train(trainloader, model, loss_fn, optimizer, iter)
    
    ##Used the internet for support with this validation logging
    while (iter[0] // 1000) > (len(result_plot)):
        
        print(f"\n--- Validation at Step {iter[0]} ---")
        
        # Run validation and get the loss/accuracy metrics
        val_loss, val_acc = test(testloader, model, loss_fn) 
        
        # Store the results
        result_plot.append({
            'step': iter[0],
            'loss': val_loss,
            'accuracy': 1.0 - val_acc
        })
        print(f"Validation reported for step {iter[0]}.")
print("TRAINING COMPLETE.")



# --- PLOTTING LOGIC ---
# Plot logic written with assistance from copilot
# 1. Extract data from the result_plot list
steps = [d['step'] for d in result_plot]
losses = [d['loss'] for d in result_plot]
errors = [d['error'] for d in result_plot] # This assumes you fixed the loop to store 'error'

# 2. Create the plots
plt.figure(figsize=(10, 5))

# Plot 1: Validation Loss vs. Weight Updates
plt.subplot(1, 2, 1) # 1 row, 2 columns, 1st plot
plt.plot(steps, losses, marker='o', linestyle='-', color='b')
plt.title('Validation Loss vs. Weight Updates')
plt.xlabel('Weight Updates (Global Step)')
plt.ylabel('Average Cross-Entropy Loss')
plt.grid(True)

# Plot 2: Validation Error vs. Weight Updates
plt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd plot
plt.plot(steps, errors, marker='o', linestyle='-', color='r')
plt.title('Validation Error vs. Weight Updates')
plt.xlabel('Weight Updates (Global Step)')
plt.ylabel('Error Rate (1 - Accuracy)')
plt.grid(True)

# Adjust layout to prevent overlap
plt.tight_layout() 

# 3. Display the plots
plt.show()

#Saving the model
#torch.save(model.state_dict(), "allcnn_cifar10.pth")
#print("Saved PyTorch Model State to allcnn_cifar10.pth")
import os
output_dir = "/content/drive/MyDrive/ESE5460_HW2_outputs/"
os.makedirs(output_dir, exist_ok=True)
filename = f"allcnn_cifar10_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth"
torch.save(model.state_dict(), os.path.join(output_dir, filename))
print("Saved model to Google Drive!")
